{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Lymphoma**"
      ],
      "metadata": {
        "id": "qYBxGL4WBYMv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CivO4iNiBI1Q"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Concatenate, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "base_path = '/content/drive/My Drive/Lymphoma'  # Update with your dataset path\n",
        "lymph_mcl_path = os.path.join(base_path, 'lymph_mcl')\n",
        "lymph_fl_path = os.path.join(base_path, 'lymph_fl')\n",
        "lymph_cll_path = os.path.join(base_path, 'lymph_cll')\n",
        "\n",
        "# Print the contents of the folders to verify paths\n",
        "print(\"Contents of lymph_mcl folder:\")\n",
        "print(os.listdir(lymph_mcl_path))\n",
        "\n",
        "print(\"Contents of lymph_fl folder:\")\n",
        "print(os.listdir(lymph_fl_path))\n",
        "\n",
        "print(\"Contents of lymph_cll folder:\")\n",
        "print(os.listdir(lymph_cll_path))\n",
        "\n",
        "# Parameters\n",
        "input_shape = (256, 256, 3)  # Resize images to 256x256\n",
        "num_classes = 3\n",
        "\n",
        "# Load and preprocess images\n",
        "def load_images_from_folder(folder, label, size):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for ext in ('*.jpg', '*.jpeg', '*.png'):\n",
        "        for filename in glob(os.path.join(folder, ext)):\n",
        "            try:\n",
        "                img = Image.open(filename).resize(size)\n",
        "                img = np.array(img)\n",
        "                if img is not None and len(img.shape) == 3 and img.shape[2] == 3:\n",
        "                    images.append(img)\n",
        "                    labels.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {filename}: {e}\")\n",
        "    print(f'Loaded {len(images)} images from {folder}')\n",
        "    return images, labels\n",
        "\n",
        "lymph_mcl_images, lymph_mcl_labels = load_images_from_folder(lymph_mcl_path, 0, (256, 256))\n",
        "lymph_fl_images, lymph_fl_labels = load_images_from_folder(lymph_fl_path, 1, (256, 256))\n",
        "lymph_cll_images, lymph_cll_labels = load_images_from_folder(lymph_cll_path, 2, (256, 256))\n",
        "\n",
        "# Combine and split the data\n",
        "images = np.array(lymph_mcl_images + lymph_fl_images + lymph_cll_images)\n",
        "labels = np.array(lymph_mcl_labels + lymph_fl_labels + lymph_cll_labels)\n",
        "\n",
        "print(f'Total images: {len(images)}')\n",
        "print(f'Total labels: {len(labels)}')\n",
        "\n",
        "if len(images) == 0:\n",
        "    raise ValueError(\"No images loaded. Please check the dataset paths and file extensions.\")\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Pyramid pooling module function\n",
        "def build_ippm(input_tensor, pool_sizes):\n",
        "    pyramid_pooling_outputs = []\n",
        "    for pool_size in pool_sizes:\n",
        "        x = tf.image.resize(input_tensor,\n",
        "                            (input_tensor.shape[1] * pool_size, input_tensor.shape[2] * pool_size))\n",
        "        x = MaxPooling2D(pool_size)(x)\n",
        "        x = Conv2D(128, (1, 1), activation='relu')(x)\n",
        "        pyramid_pooling_outputs.append(x)\n",
        "    return Concatenate()(pyramid_pooling_outputs)\n",
        "\n",
        "# Backbone function\n",
        "def backbone(input_shape, num_classes, pool_sizes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    base_model = VGG19(include_top=False, input_tensor=inputs)\n",
        "\n",
        "    x = build_ippm(base_model.output, pool_sizes)\n",
        "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Initialize the model\n",
        "pool_sizes = [2, 3, 4, 6]\n",
        "model = backbone(input_shape, num_classes, pool_sizes)\n",
        "\n",
        "# Compile the model\n",
        "learning_rate = 0.00001\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=8, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f'Test accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Oral Cancer**"
      ],
      "metadata": {
        "id": "21yOn1f5BS-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, GlobalAveragePooling2D, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define paths\n",
        "base_path = '/content/drive/My Drive/Oral Cancer'  # Update with your dataset path\n",
        "oral_scc_path = os.path.join(base_path, 'oral_scc')\n",
        "oral_normal_path = os.path.join(base_path, 'oral_normal')\n",
        "\n",
        "# Print the contents of the folders to verify paths\n",
        "print(\"Contents of oral_scc folder:\")\n",
        "print(os.listdir(oral_scc_path))\n",
        "\n",
        "print(\"Contents of oral_normal folder:\")\n",
        "print(os.listdir(oral_normal_path))\n",
        "\n",
        "# Parameters\n",
        "input_shape = (256, 256, 3)  # Resize images to 256x256\n",
        "num_classes = 2\n",
        "\n",
        "# Load and preprocess images\n",
        "def load_images_from_folder(folder, label, size):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for ext in ('*.jpg', '*.jpeg', '*.png'):\n",
        "        for filename in glob(os.path.join(folder, ext)):\n",
        "            try:\n",
        "                img = Image.open(filename).resize(size)\n",
        "                img = np.array(img)\n",
        "                if img is not None and len(img.shape) == 3 and img.shape[2] == 3:\n",
        "                    images.append(img)\n",
        "                    labels.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {filename}: {e}\")\n",
        "    print(f'Loaded {len(images)} images from {folder}')\n",
        "    return images, labels\n",
        "\n",
        "oral_scc_images, oral_scc_labels = load_images_from_folder(oral_scc_path, 0, (256, 256))\n",
        "oral_normal_images, oral_normal_labels = load_images_from_folder(oral_normal_path, 1, (256, 256))\n",
        "\n",
        "# Combine and split the data\n",
        "images = np.array(oral_scc_images + oral_normal_images)\n",
        "labels = np.array(oral_scc_labels + oral_normal_labels)\n",
        "\n",
        "print(f'Total images: {len(images)}')\n",
        "print(f'Total labels: {len(labels)}')\n",
        "\n",
        "if len(images) == 0:\n",
        "    raise ValueError(\"No images loaded. Please check the dataset paths and file extensions.\")\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def create_mobilenetv2_model(input_shape, num_classes):\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Initialize the model\n",
        "model = create_mobilenetv2_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=25, batch_size=8, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f'Test accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VMSQFoUJBN_n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}